{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADAGRAD-challenge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMikOcjsSXdf"
      },
      "source": [
        "####**The challenge** -- Use a pre-trained ResNet-18 model and implement a custom reduction operator in ONNX C++ to perform image similarity on the given dataset\n",
        "\n",
        "\n",
        "* what is deformable network\n",
        "* what is ONNX and how to use it, why c++ with ONNX\n",
        "* what is output maps in CNN.\n",
        "\n",
        "\n",
        "TODO:\n",
        "\n",
        "* Create a model called ReductionResNET-18 -> can use pretrained from torchvison.\n",
        "* what is reduction operator the C++ part\n",
        "* \n",
        "\n",
        "MY APPROACH :\n",
        "\n",
        "First i will read about the resnet-18 architechture what it is made of.\n",
        "than we will take a simple resnet-18 model. and perform transfer learning on given data set.\n",
        "\n",
        "\n",
        "\n",
        "RESEARCH:\n",
        "\n",
        "Link: https://medium.com/swlh/5-pytorch-functions-for-reduction-operations-b25aa1e208a3\n",
        "https://shuzhanfan.github.io/2018/11/ResNet/\n",
        "deformable convolution networks\n",
        "https://arxiv.org/abs/1703.06211\n",
        "extract featrues from intermediate layers of pytroch pretrained resnet-18 model\n",
        "\n",
        "https://medium.com/the-owl/extracting-features-from-an-intermediate-layer-of-a-pretrained-model-in-pytorch-c00589bda32b\n",
        "\n",
        "read for getting a blok output.\n",
        "https://towardsdatascience.com/everything-you-need-to-know-about-saving-weights-in-pytorch-572651f3f8de\n",
        "\n",
        "this might help me\n",
        "https://stackoverflow.com/questions/63427771/extracting-intermediate-layer-outputs-of-a-cnn-in-pytorch\n",
        "\n",
        "torchextractor might help in feature extraction.\n",
        "https://github.com/antoinebrl/torchextractor.git\n",
        "\n",
        "\n",
        "findings :\n",
        "\n",
        "module subclassing.\n",
        "\n",
        "\n",
        "model.summary\n",
        "https://medium.com/the-owl/extracting-features-from-an-intermediate-layer-of-a-pretrained-model-in-pytorch-c00589bda32b\n",
        "\n",
        "https://towardsdatascience.com/recommending-similar-images-using-pytorch-da019282770c\n",
        "\n",
        "\n",
        "for pytorch hooks\n",
        "https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbl8mNGESPQM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9_jif2NbJP4"
      },
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "resnet18 = models.resnet18()\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLzpnjyJbNzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b3ab9e-be07-4838-e848-f40c9b8e504d"
      },
      "source": [
        "dir(resnet18)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T_destination',\n",
              " '__annotations__',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_apply',\n",
              " '_backward_hooks',\n",
              " '_buffers',\n",
              " '_call_impl',\n",
              " '_forward_hooks',\n",
              " '_forward_impl',\n",
              " '_forward_pre_hooks',\n",
              " '_get_backward_hooks',\n",
              " '_get_name',\n",
              " '_is_full_backward_hook',\n",
              " '_load_from_state_dict',\n",
              " '_load_state_dict_pre_hooks',\n",
              " '_make_layer',\n",
              " '_maybe_warn_non_full_backward_hook',\n",
              " '_modules',\n",
              " '_named_members',\n",
              " '_non_persistent_buffers_set',\n",
              " '_norm_layer',\n",
              " '_parameters',\n",
              " '_register_load_state_dict_pre_hook',\n",
              " '_register_state_dict_hook',\n",
              " '_replicate_for_data_parallel',\n",
              " '_save_to_state_dict',\n",
              " '_slow_forward',\n",
              " '_state_dict_hooks',\n",
              " '_version',\n",
              " 'add_module',\n",
              " 'apply',\n",
              " 'avgpool',\n",
              " 'base_width',\n",
              " 'bfloat16',\n",
              " 'bn1',\n",
              " 'buffers',\n",
              " 'children',\n",
              " 'conv1',\n",
              " 'cpu',\n",
              " 'cuda',\n",
              " 'dilation',\n",
              " 'double',\n",
              " 'dump_patches',\n",
              " 'eval',\n",
              " 'extra_repr',\n",
              " 'fc',\n",
              " 'float',\n",
              " 'forward',\n",
              " 'get_buffer',\n",
              " 'get_parameter',\n",
              " 'get_submodule',\n",
              " 'groups',\n",
              " 'half',\n",
              " 'inplanes',\n",
              " 'layer1',\n",
              " 'layer2',\n",
              " 'layer3',\n",
              " 'layer4',\n",
              " 'load_state_dict',\n",
              " 'maxpool',\n",
              " 'modules',\n",
              " 'named_buffers',\n",
              " 'named_children',\n",
              " 'named_modules',\n",
              " 'named_parameters',\n",
              " 'parameters',\n",
              " 'register_backward_hook',\n",
              " 'register_buffer',\n",
              " 'register_forward_hook',\n",
              " 'register_forward_pre_hook',\n",
              " 'register_full_backward_hook',\n",
              " 'register_parameter',\n",
              " 'relu',\n",
              " 'requires_grad_',\n",
              " 'share_memory',\n",
              " 'state_dict',\n",
              " 'to',\n",
              " 'to_empty',\n",
              " 'train',\n",
              " 'training',\n",
              " 'type',\n",
              " 'xpu',\n",
              " 'zero_grad']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhRZvoS1bNtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbdbd813-4f94-4c11-b1eb-0ffa1c636e20"
      },
      "source": [
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "def image_loader(loader, image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = loader(image).float()\n",
        "    image = torch.tensor(image, requires_grad=True)\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(440),\n",
        "    transforms.CenterCrop(416),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "model_ft.eval()\n",
        "FILENAME=\"/content/pexels-dzenina-lukac-1583884.jpg\"\n",
        "print( np.argmax(model_ft(image_loader(data_transforms, FILENAME)).detach().numpy()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7osKEN0LbNwY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNIr-mVhbyG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w6u_3uebNp3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c98f27-8ec6-4337-c284-bdf2d527c11b"
      },
      "source": [
        "model_ft(image_loader(data_transforms, FILENAME))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 7.3491e-01,  2.7160e+00,  7.9677e-01, -8.2283e-01,  1.6049e+00,\n",
              "          1.7790e+00, -4.7164e-01,  4.2707e-01,  9.5064e-01, -8.9202e-01,\n",
              "         -2.5634e-01, -1.6432e+00,  3.2216e-01,  6.8254e-01,  1.1927e-01,\n",
              "          4.1155e-02,  9.3419e-01,  7.9108e-01,  8.2697e-01,  1.3626e+00,\n",
              "         -3.6341e-01, -8.1614e-01, -2.1467e+00, -3.0721e-02, -2.4887e+00,\n",
              "         -2.6393e-01,  3.4347e+00,  1.8787e+00,  1.1364e+00,  5.0717e+00,\n",
              "         -7.9001e-01,  5.9446e-01,  9.0946e-01,  1.6798e+00,  1.1652e+00,\n",
              "          1.3570e+00,  2.1304e+00,  1.1955e+00,  1.9301e+00,  3.1919e-01,\n",
              "          2.1331e+00,  2.6146e-01,  2.3447e+00,  1.6853e+00,  1.1560e+00,\n",
              "          8.6508e-01,  1.4410e+00,  9.7460e-01, -1.6531e+00, -7.9070e-01,\n",
              "          4.6521e-01,  7.1298e-01,  2.6040e+00,  3.4371e-04,  1.2697e+00,\n",
              "         -1.0296e-01,  2.4206e+00, -4.9466e-01,  2.7146e+00,  6.4679e-01,\n",
              "          1.1320e+00,  1.2463e+00,  5.4707e-01,  2.0054e+00,  1.7773e+00,\n",
              "          4.1954e-01,  2.7912e+00,  2.1097e+00,  1.6156e+00,  1.7292e+00,\n",
              "          1.2575e+00,  3.8027e+00,  3.9390e-01,  1.1919e+00,  9.9275e-01,\n",
              "          1.5182e+00,  6.1137e-01,  2.0120e+00,  2.6468e+00,  3.6928e+00,\n",
              "          8.0495e-01,  6.1947e-01, -1.9654e-01,  7.3339e-01, -6.9670e-01,\n",
              "          3.5116e-01,  1.0468e+00,  4.5908e-01, -1.3991e+00,  3.2145e-01,\n",
              "         -1.0438e+00,  1.6307e+00, -4.7681e-01, -8.9174e-02,  1.4051e+00,\n",
              "          3.4221e-01, -1.3559e+00, -9.3774e-01, -1.7781e+00,  4.7733e-01,\n",
              "          6.1366e-01, -2.2327e+00, -6.4855e-01,  2.2698e-01, -2.1383e+00,\n",
              "         -1.6828e+00, -1.7558e+00,  8.4946e-01,  4.1295e+00,  2.5931e+00,\n",
              "          7.0052e-01,  2.9199e+00,  1.8878e+00,  3.5307e+00,  2.4562e+00,\n",
              "          4.3836e+00,  1.8252e+00,  2.0430e+00,  3.6098e+00,  2.2633e+00,\n",
              "          3.7088e+00,  2.1386e+00,  2.1900e+00,  3.2004e-01,  4.1937e+00,\n",
              "          3.0805e+00,  4.2195e+00, -1.6963e+00, -2.2071e+00, -2.0045e+00,\n",
              "         -1.8747e+00, -1.5755e+00, -4.0629e-01, -2.8580e-01, -1.3586e+00,\n",
              "         -4.1753e-01,  1.8219e-01, -7.5023e-01, -5.7390e-01, -1.1892e-01,\n",
              "         -9.6406e-01, -1.0566e+00, -7.9687e-01, -3.7516e-02, -1.8620e+00,\n",
              "         -1.0652e+00, -3.1311e-01, -2.0982e+00,  4.5271e-01, -2.1848e+00,\n",
              "         -5.2781e-01,  1.6001e+00, -4.2321e-01, -8.3023e-01,  1.1596e+00,\n",
              "         -9.6705e-01, -2.0369e+00, -1.6783e+00,  5.6800e-01, -1.5430e+00,\n",
              "         -1.3049e+00, -1.7370e+00, -5.1995e-02, -6.7921e-01, -1.4925e+00,\n",
              "         -1.4446e+00, -1.0525e+00, -1.0685e+00, -3.5365e-02, -2.2903e+00,\n",
              "         -2.9226e+00, -2.4496e-01,  9.0681e-02, -6.0146e-01,  6.6612e-01,\n",
              "         -2.1982e+00, -2.0809e+00, -2.6834e+00, -6.9931e-01,  3.6751e-02,\n",
              "         -8.9187e-01, -7.2449e-01, -1.1858e+00, -4.9934e-01, -1.6805e+00,\n",
              "         -3.0354e-01,  3.7404e-01, -4.6813e-01, -1.0273e+00, -3.8153e-01,\n",
              "         -1.1327e+00, -1.8757e+00, -5.9215e-01, -3.3892e-01, -5.7278e-01,\n",
              "          9.4364e-01, -1.2575e+00, -1.3881e+00, -1.3231e+00,  2.3132e-01,\n",
              "         -1.1685e+00,  2.8365e-01, -6.1175e-01, -1.1653e-01, -7.9409e-01,\n",
              "         -1.6589e+00, -1.3002e+00, -2.3170e-01,  9.0191e-01, -1.0826e+00,\n",
              "         -1.7593e+00, -4.2648e-01, -2.0365e+00, -1.3637e+00, -9.2222e-01,\n",
              "         -2.3336e+00, -2.6286e+00, -1.6708e+00, -2.5947e+00, -1.2483e+00,\n",
              "         -1.4525e+00, -1.5658e+00, -8.7305e-01,  2.3371e-01, -4.1652e-01,\n",
              "         -1.0722e+00, -3.9952e-01, -4.0111e-01, -1.3726e+00, -2.4911e+00,\n",
              "         -1.1760e+00, -7.1258e-01, -3.2346e-01, -1.1574e+00,  2.5469e-01,\n",
              "          9.0112e-01,  4.7998e-01,  1.0677e+00, -8.3132e-01, -1.4201e+00,\n",
              "          1.3211e-01, -1.0518e+00, -1.3209e+00, -1.5982e+00, -1.0976e+00,\n",
              "          4.7718e-01, -7.9431e-01, -1.1675e+00,  1.8311e+00,  5.2076e-01,\n",
              "          8.9974e-01, -5.5895e-01, -8.3202e-01,  6.5217e-01, -8.1190e-02,\n",
              "         -3.0973e+00, -5.2144e-01, -8.4938e-01, -4.4330e-01,  9.8882e-01,\n",
              "         -6.2946e-01, -1.1437e+00, -7.9249e-01, -4.8132e-01, -8.5315e-01,\n",
              "          9.8095e-01, -7.9164e-03,  2.2823e-02, -6.9765e-01, -1.2569e-01,\n",
              "         -4.3044e-01, -9.4190e-01,  2.1746e-01,  6.2475e-01, -1.4420e+00,\n",
              "         -2.2017e+00, -2.6568e+00, -7.3979e-01,  4.8266e-01,  1.4060e-01,\n",
              "         -2.5548e-01, -1.5120e-01, -5.6939e-01, -9.6876e-01,  1.7542e-01,\n",
              "         -1.1785e-01, -1.5203e+00, -1.4051e+00, -1.3963e+00, -2.5694e+00,\n",
              "         -1.7412e+00, -3.0333e+00, -1.9571e+00, -2.7005e+00, -2.5647e+00,\n",
              "         -1.5198e+00, -4.0634e-01, -1.8055e+00,  4.7717e-01,  5.6110e-01,\n",
              "          1.6163e-01,  4.6525e+00,  1.6281e+00,  2.0932e+00,  2.1071e+00,\n",
              "          1.4973e+00,  2.6417e+00,  3.3738e+00,  1.5937e+00,  3.1187e+00,\n",
              "          4.0261e+00,  2.2597e+00,  3.0672e+00,  2.3170e-01,  4.9250e+00,\n",
              "          1.7076e+00,  7.1159e-01,  2.2681e+00,  1.1125e+00,  8.4427e-01,\n",
              "          7.6094e-01,  6.1525e-01,  1.0032e+00,  5.8896e-01,  1.4325e+00,\n",
              "          1.2762e+00,  1.4112e+00,  4.0953e+00,  2.1245e+00,  2.6131e+00,\n",
              "          5.7983e-01, -1.0292e-01, -2.7078e-01,  2.3383e+00, -1.3084e+00,\n",
              "         -2.5478e+00, -1.1777e+00, -1.6745e+00, -1.4830e+00, -2.6853e+00,\n",
              "         -2.2755e+00, -2.2082e+00, -2.5140e+00, -3.6985e+00, -2.1659e+00,\n",
              "         -2.1026e+00, -2.1494e+00, -3.5648e+00, -2.0674e+00, -2.1359e+00,\n",
              "         -1.1938e+00, -2.0964e+00, -1.6222e+00, -1.0723e+00, -2.6676e+00,\n",
              "         -2.7290e-01,  5.8278e-01, -2.8776e-01, -7.3694e-02, -6.5071e-01,\n",
              "         -1.1776e+00, -1.1055e+00, -7.5805e-01,  8.3015e-01, -9.4437e-01,\n",
              "          2.6497e-02, -1.4670e+00, -1.7208e+00,  2.1546e-01, -5.1231e-01,\n",
              "         -1.3805e+00, -7.4758e-01, -1.3310e+00, -1.5717e+00, -1.9287e-01,\n",
              "         -1.8758e+00, -1.2925e+00, -3.2091e-01, -1.1842e+00,  8.0699e-01,\n",
              "          6.8376e-01, -3.0495e-01,  3.6402e-01, -2.9898e+00, -6.8207e-01,\n",
              "         -2.5903e+00, -1.9753e+00, -7.5624e-01, -2.7342e+00,  4.4063e-01,\n",
              "          1.2470e+00,  3.3089e-01, -9.0514e-01,  2.6064e+00,  5.0038e-01,\n",
              "          1.2121e+00,  8.8279e-01,  1.4829e+00,  1.9078e+00,  2.3423e-01,\n",
              "         -8.6472e-01, -2.6540e-01, -2.2628e+00, -3.3273e+00, -1.8065e+00,\n",
              "         -5.3372e-01, -9.4558e-01, -3.2700e+00, -2.2273e+00,  7.9479e-02,\n",
              "          1.0646e-01, -4.8466e-01, -4.5000e-01, -2.4102e+00, -1.3439e+00,\n",
              "          1.4147e+00, -7.3402e-01, -1.2175e+00,  7.6271e-01,  2.6817e+00,\n",
              "         -7.6453e-01, -7.1602e-01, -2.0327e+00, -1.2694e+00, -5.4807e-01,\n",
              "         -1.4376e+00, -2.1458e+00,  6.3732e-02, -2.8213e-02,  6.2284e-03,\n",
              "         -1.5280e+00,  1.2365e+00, -1.2978e+00, -2.3838e-01,  3.1557e+00,\n",
              "          2.8623e+00, -2.1366e+00, -9.6686e-01,  6.8750e-01, -9.1182e-01,\n",
              "          3.6694e-01,  4.8565e-01, -1.9983e+00,  2.1451e-01, -3.5402e+00,\n",
              "         -8.5136e-01,  8.5221e-01, -2.7515e+00,  3.8926e-02, -1.7874e+00,\n",
              "         -1.2610e+00, -1.9389e-01,  5.0626e-01, -9.4892e-01,  4.8370e-01,\n",
              "          1.7198e+00, -1.7574e+00,  1.9078e+00, -1.5769e+00,  1.0878e+00,\n",
              "         -1.1653e+00, -1.5590e+00, -4.3302e-01,  2.8868e+00,  1.7486e-01,\n",
              "         -1.2715e+00, -2.1789e+00,  2.3562e-01, -2.9152e+00,  2.5722e+00,\n",
              "          4.1792e+00, -1.4516e+00, -1.4913e+00,  9.7360e-01, -2.2213e-01,\n",
              "         -1.6558e+00, -8.5268e-01, -8.1016e-01,  1.2802e+00,  1.1958e-01,\n",
              "         -1.4522e-01, -9.7425e-01, -2.5671e+00, -2.3976e+00, -2.7357e+00,\n",
              "         -2.7374e+00, -2.2728e+00,  1.6298e+00,  2.2245e+00,  1.0289e+00,\n",
              "          3.3401e-01, -3.3263e+00, -2.0410e-01, -7.9207e-01, -3.2424e-01,\n",
              "         -7.9795e-01, -8.8481e-01, -1.3909e+00, -5.8027e-01,  1.3663e+00,\n",
              "         -1.7237e+00, -6.7276e-01,  1.1191e+00,  3.8913e-01,  2.3293e+00,\n",
              "          1.2355e+00,  3.0425e+00, -9.0293e-02,  4.0268e+00,  2.3014e+00,\n",
              "          4.4933e-01, -1.8713e+00,  5.4318e-02, -1.8883e+00, -1.1361e+00,\n",
              "          1.3723e+00,  2.1560e+00, -1.5789e+00, -1.5255e+00,  6.8507e-01,\n",
              "          1.4180e+00,  3.3182e+00, -4.2470e-01, -1.5097e+00, -1.5275e+00,\n",
              "         -1.2788e+00, -5.1060e-01, -1.0841e+00,  1.3269e+00,  7.2110e-01,\n",
              "         -3.3746e-01, -2.1879e+00, -4.1257e-01,  7.1776e-01,  1.9334e-01,\n",
              "         -1.6489e+00, -2.1230e+00, -5.1663e-01,  2.8310e-01, -8.6120e-02,\n",
              "         -2.1557e+00,  6.3011e-02,  2.4446e+00, -1.1777e+00,  1.6043e+00,\n",
              "         -3.3496e-01, -2.0333e+00, -4.5308e+00, -1.9750e+00,  2.4108e+00,\n",
              "         -5.5170e-01,  1.4850e+00, -1.3764e-02, -1.3888e+00, -6.2723e-01,\n",
              "         -3.0222e+00, -8.5605e-01, -1.7499e+00, -1.6905e-01, -9.6209e-01,\n",
              "         -1.0517e+00, -2.6719e+00,  5.1138e-01,  3.0279e-01, -1.0471e+00,\n",
              "         -2.4480e+00, -9.3142e-01,  3.0393e+00, -5.7009e-01, -2.6771e+00,\n",
              "         -1.4362e+00, -3.9993e+00,  1.5653e+00, -2.3078e+00,  1.8047e+00,\n",
              "         -1.6577e+00, -2.3591e+00, -1.2238e-01, -4.8643e-01, -3.5271e-01,\n",
              "         -1.8968e-01, -2.4805e+00,  2.6842e+00, -2.3490e+00,  2.3473e+00,\n",
              "          4.1066e-02, -1.7478e+00, -6.6478e-01,  2.3019e+00,  8.8808e-02,\n",
              "         -1.5853e-01,  1.6381e+00, -3.0089e+00, -1.4143e+00, -8.4656e-01,\n",
              "         -1.5148e+00, -9.5176e-01, -2.7453e+00, -1.6880e+00,  3.5757e+00,\n",
              "          3.8601e-01,  7.5956e-01, -4.2914e-01, -1.8020e+00,  6.5472e-01,\n",
              "          6.8575e-01,  1.3875e-01,  1.4546e+00, -1.1970e+00, -2.0522e+00,\n",
              "         -1.9607e+00,  2.5514e-01, -3.5575e+00, -9.5171e-01, -1.4368e+00,\n",
              "         -3.1429e+00,  2.8985e+00,  6.0113e-01,  4.2976e+00,  8.5556e-01,\n",
              "          2.2052e+00, -4.2143e+00,  8.1480e-01,  2.1997e+00,  3.5398e-01,\n",
              "         -2.7816e+00, -4.7643e-01, -2.7563e+00, -2.0920e+00,  2.1146e+00,\n",
              "         -2.2085e-01,  3.5100e+00, -1.5587e+00, -8.7061e-03, -1.3371e+00,\n",
              "         -2.3296e-01, -4.7435e-02, -1.8297e+00, -1.3397e+00, -1.4417e+00,\n",
              "         -1.8517e+00,  1.7731e+00, -9.5474e-01, -7.6427e-01,  5.4651e+00,\n",
              "         -2.2581e+00,  1.5908e+00,  2.0028e+00,  4.8438e-01, -2.7670e+00,\n",
              "          1.7798e-01, -7.5639e-01, -2.0234e+00, -2.2189e-02, -2.7724e+00,\n",
              "         -7.3025e-01, -1.7052e+00, -5.0090e-01,  6.9059e-01,  4.7411e+00,\n",
              "         -3.1678e+00, -2.3529e+00, -7.8661e-01, -1.4704e+00,  8.4175e-02,\n",
              "         -2.0893e+00,  5.6653e+00,  1.0530e-02, -2.0359e+00,  7.3587e-01,\n",
              "         -2.3655e+00, -2.4316e+00, -2.5726e+00,  5.2199e-01,  4.3994e-01,\n",
              "         -2.6132e+00,  1.9108e+00,  2.1402e+00, -1.4920e+00,  1.4249e+00,\n",
              "          1.6544e+00,  2.4965e+00,  1.9735e-01, -1.0231e+00,  1.1178e-01,\n",
              "         -2.4064e+00, -6.6221e-01, -1.4444e+00, -2.2372e+00, -4.5743e-01,\n",
              "         -2.6977e+00, -1.0653e+00,  3.3564e+00, -3.2232e-01,  1.0797e+00,\n",
              "         -1.0239e+00,  8.8211e-01, -1.4032e+00, -1.3777e+00,  1.6866e+00,\n",
              "          3.4337e+00, -3.9836e-01, -1.4711e-01, -1.7987e+00, -1.2817e+00,\n",
              "         -3.2919e+00, -1.2677e+00, -2.0312e+00, -2.4242e-01, -5.9628e-01,\n",
              "          4.6919e-01,  1.4312e+00,  2.7140e+00,  3.0833e-01,  2.0771e+00,\n",
              "          8.5183e-02,  1.2033e+00, -3.0929e+00, -1.9229e+00,  1.0989e+00,\n",
              "          2.9891e+00,  4.5342e-01,  1.7023e+00,  2.7594e+00, -1.6075e+00,\n",
              "          1.1415e+00, -6.7159e-01, -2.0865e+00,  1.5665e+00, -8.1645e-01,\n",
              "         -3.9149e-01,  9.3842e-01, -1.0125e+00, -2.2370e+00, -3.2385e+00,\n",
              "         -6.9208e-01, -1.9229e+00, -1.1575e+00,  3.0513e+00,  6.3448e-01,\n",
              "         -2.6953e+00, -2.4070e+00, -4.5678e-01, -2.2625e+00, -2.4657e-01,\n",
              "         -1.1169e+00,  7.2813e-01, -2.4399e+00,  1.4926e+00,  1.5241e+00,\n",
              "          1.0996e+00, -3.4881e+00, -1.7032e+00, -6.4562e-01, -1.4377e+00,\n",
              "          5.7591e-01,  9.5384e-01, -3.1612e+00, -1.3812e+00, -2.0314e+00,\n",
              "          4.4438e-01, -1.9476e-02,  1.6537e+00, -2.1638e+00, -2.8907e+00,\n",
              "          1.2579e+00, -1.4843e+00,  2.5774e+00, -2.9174e+00,  1.4979e+00,\n",
              "          2.6812e-01, -1.8356e+00,  2.0059e+00,  1.8437e+00,  3.5809e-01,\n",
              "         -2.1348e+00, -2.5365e+00, -8.9208e-01, -1.7939e-01, -3.1611e+00,\n",
              "         -8.9555e-01, -2.7262e+00,  1.2557e+00,  1.5804e+00, -5.9594e-01,\n",
              "         -2.8742e-01, -1.3512e+00,  9.2789e-01, -7.4927e-02,  3.1146e-02,\n",
              "          7.9649e-01, -7.5479e-01,  9.5712e-02,  1.8773e+00,  2.2082e+00,\n",
              "         -2.7323e+00, -6.2729e-01, -8.8736e-01, -1.3913e+00, -1.5400e+00,\n",
              "         -3.2783e+00, -9.4242e-01, -1.7193e+00, -2.3400e+00,  1.2245e+00,\n",
              "         -3.4683e-01, -2.7636e-01, -1.5972e+00,  1.0773e+00,  6.9407e+00,\n",
              "          2.9984e+00, -9.7502e-01, -1.0343e+00,  2.9035e+00, -1.8257e+00,\n",
              "          3.6638e-01,  1.8139e+00, -2.7840e+00,  6.5102e-01, -1.9102e+00,\n",
              "         -4.6946e+00, -2.3986e+00, -1.8932e-01, -3.3723e-01,  7.0977e-01,\n",
              "          1.3152e+00, -4.5108e-01,  1.5844e-01,  4.1232e+00, -3.7997e+00,\n",
              "         -3.2354e+00,  1.2868e+00, -3.0988e-01, -2.9550e+00, -4.8578e-02,\n",
              "          4.7998e-01, -6.4149e-01, -1.3706e-01,  1.6394e+00, -3.5690e+00,\n",
              "          3.6144e-01,  5.3675e-02, -6.6611e-01, -1.2866e+00, -6.1318e-01,\n",
              "          2.1756e+00, -2.8794e-02, -2.4536e+00, -1.7698e+00,  2.7255e+00,\n",
              "          1.4183e+00, -8.5137e-01,  1.2024e+00, -3.9873e-01, -7.6340e-01,\n",
              "          2.3515e+00, -1.7550e+00, -2.4535e+00,  1.2235e+00,  9.5337e-03,\n",
              "         -1.4920e-01,  1.9650e+00, -5.8442e-01, -2.8081e+00, -3.6522e+00,\n",
              "          8.3830e-02, -1.6382e+00, -3.2120e+00,  3.1705e+00, -3.0787e-02,\n",
              "         -1.5633e+00, -2.2839e+00, -5.9898e-01, -3.1362e+00, -4.5095e+00,\n",
              "         -1.9649e+00,  3.2858e+00, -3.0050e+00,  2.2529e+00, -2.1633e-01,\n",
              "         -2.3137e+00, -5.0263e-01, -1.2517e+00,  1.9357e+00, -1.3249e+00,\n",
              "          2.9384e+00, -9.3464e-01, -2.3910e-01, -3.7470e+00, -2.4006e+00,\n",
              "         -1.2800e+00,  2.3914e+00,  3.9161e-01, -1.0227e+00, -1.1746e+00,\n",
              "         -7.0126e-01,  4.0503e-01,  5.3832e-01,  1.0885e-01, -2.5292e-01,\n",
              "         -2.2570e+00, -5.1206e-02, -5.0811e-01,  9.2403e-01,  6.9499e-01,\n",
              "         -5.2918e-01,  2.2586e-01,  1.2380e+00,  5.9824e-01,  4.0102e+00,\n",
              "          5.7189e+00,  3.0901e+00,  4.5874e-01, -1.7781e-01, -1.0407e+00,\n",
              "         -1.8153e+00,  2.6871e-01, -1.1721e+00,  1.3779e+00, -2.2122e+00,\n",
              "         -2.1476e+00,  2.1635e+00,  1.9828e+00,  4.9702e+00,  3.6763e+00,\n",
              "          6.6701e+00,  4.2036e+00,  4.2663e+00,  3.5403e+00,  3.7442e+00,\n",
              "          2.6616e+00,  2.3698e+00,  3.0852e+00,  3.4918e+00,  4.0961e+00,\n",
              "          4.3499e+00,  3.0588e+00,  3.0475e+00,  2.7297e+00,  1.7851e+00,\n",
              "          3.0831e+00,  2.4587e+00,  3.6254e+00,  3.7649e+00,  2.7561e+00,\n",
              "          2.5930e+00,  1.2844e+00,  4.8678e+00,  3.9744e+00,  5.1986e+00,\n",
              "          4.7472e+00,  4.2446e+00,  3.7528e+00,  5.9151e+00,  5.3804e+00,\n",
              "          2.5996e+00,  2.6640e+00,  4.2914e+00, -4.2201e-01,  5.3108e+00,\n",
              "          3.4072e+00,  4.2410e+00,  3.0459e+00, -2.0409e-01,  3.2247e+00,\n",
              "          3.1999e+00,  2.2199e+00,  2.9736e+00,  4.4244e+00,  3.5585e+00,\n",
              "          8.4787e-02, -3.5585e-01, -1.4388e+00,  1.9892e+00, -2.5580e+00,\n",
              "          1.4629e-01, -9.5493e-01,  1.6631e-01, -7.6810e-01, -8.3869e-01,\n",
              "          3.1758e-02, -1.1871e+00,  1.7920e-01, -1.1036e+00,  3.2812e-01,\n",
              "          3.7986e+00,  6.5231e-01,  5.8213e+00,  1.6162e+00,  3.4508e+00,\n",
              "          2.2198e+00,  2.9692e+00,  2.0470e+00,  1.8208e+00,  3.1711e+00,\n",
              "          3.6710e+00,  3.7346e+00,  3.1375e-01,  4.9480e+00,  2.8258e+00]],\n",
              "       grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Athp_VPbNnE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "1cf0e6b0-51cc-4a86-cf9c-6c5dbd06a1fc"
      },
      "source": [
        "c = model_ft.layer1(image_loader(data_transforms, FILENAME)).detach().numpy()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9ae1c8b5b12d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 64, 3, 3], expected input[1, 3, 416, 416] to have 64 channels, but got 3 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e6Oz0LFb_vU"
      },
      "source": [
        "c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u2C13JscEzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194e145b-8f85-4c85-d773-695a057da7d9"
      },
      "source": [
        "print(resnet18.state_dict)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.state_dict of ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyYbEbNtdEja"
      },
      "source": [
        "# class MyResNet18(resnet18):\n",
        "\n",
        "#   def __init__(self, *args, **kwargs):\n",
        "#     super().__init__(*args, **kwargs)\n",
        "\n",
        "#   def forward(self, xb):\n",
        "#     x = self.maxpool(self.relu(self.bn1(self.conv1(xb))))\n",
        "#     x = self.layer1(x)\n",
        "#     x2 = x = self.layer2(x)\n",
        "#     x3 = x = self.layer3(x)\n",
        "#     x4 = x = self.layer4(x)\n",
        "#     return [x2, x3, x4]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9VTlfbWe_vx"
      },
      "source": [
        "# resnet18.register_forward_hook()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUNHdhaJkbaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba14693-7c3a-4afb-966d-7a058fc564a9"
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(resnet18,input_size=(3, 416, 416))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 208, 208]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 208, 208]             128\n",
            "              ReLU-3         [-1, 64, 208, 208]               0\n",
            "         MaxPool2d-4         [-1, 64, 104, 104]               0\n",
            "            Conv2d-5         [-1, 64, 104, 104]          36,864\n",
            "       BatchNorm2d-6         [-1, 64, 104, 104]             128\n",
            "              ReLU-7         [-1, 64, 104, 104]               0\n",
            "            Conv2d-8         [-1, 64, 104, 104]          36,864\n",
            "       BatchNorm2d-9         [-1, 64, 104, 104]             128\n",
            "             ReLU-10         [-1, 64, 104, 104]               0\n",
            "       BasicBlock-11         [-1, 64, 104, 104]               0\n",
            "           Conv2d-12         [-1, 64, 104, 104]          36,864\n",
            "      BatchNorm2d-13         [-1, 64, 104, 104]             128\n",
            "             ReLU-14         [-1, 64, 104, 104]               0\n",
            "           Conv2d-15         [-1, 64, 104, 104]          36,864\n",
            "      BatchNorm2d-16         [-1, 64, 104, 104]             128\n",
            "             ReLU-17         [-1, 64, 104, 104]               0\n",
            "       BasicBlock-18         [-1, 64, 104, 104]               0\n",
            "           Conv2d-19          [-1, 128, 52, 52]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 52, 52]             256\n",
            "             ReLU-21          [-1, 128, 52, 52]               0\n",
            "           Conv2d-22          [-1, 128, 52, 52]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 52, 52]             256\n",
            "           Conv2d-24          [-1, 128, 52, 52]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 52, 52]             256\n",
            "             ReLU-26          [-1, 128, 52, 52]               0\n",
            "       BasicBlock-27          [-1, 128, 52, 52]               0\n",
            "           Conv2d-28          [-1, 128, 52, 52]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 52, 52]             256\n",
            "             ReLU-30          [-1, 128, 52, 52]               0\n",
            "           Conv2d-31          [-1, 128, 52, 52]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 52, 52]             256\n",
            "             ReLU-33          [-1, 128, 52, 52]               0\n",
            "       BasicBlock-34          [-1, 128, 52, 52]               0\n",
            "           Conv2d-35          [-1, 256, 26, 26]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 26, 26]             512\n",
            "             ReLU-37          [-1, 256, 26, 26]               0\n",
            "           Conv2d-38          [-1, 256, 26, 26]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 26, 26]             512\n",
            "           Conv2d-40          [-1, 256, 26, 26]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 26, 26]             512\n",
            "             ReLU-42          [-1, 256, 26, 26]               0\n",
            "       BasicBlock-43          [-1, 256, 26, 26]               0\n",
            "           Conv2d-44          [-1, 256, 26, 26]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 26, 26]             512\n",
            "             ReLU-46          [-1, 256, 26, 26]               0\n",
            "           Conv2d-47          [-1, 256, 26, 26]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 26, 26]             512\n",
            "             ReLU-49          [-1, 256, 26, 26]               0\n",
            "       BasicBlock-50          [-1, 256, 26, 26]               0\n",
            "           Conv2d-51          [-1, 512, 13, 13]       1,179,648\n",
            "      BatchNorm2d-52          [-1, 512, 13, 13]           1,024\n",
            "             ReLU-53          [-1, 512, 13, 13]               0\n",
            "           Conv2d-54          [-1, 512, 13, 13]       2,359,296\n",
            "      BatchNorm2d-55          [-1, 512, 13, 13]           1,024\n",
            "           Conv2d-56          [-1, 512, 13, 13]         131,072\n",
            "      BatchNorm2d-57          [-1, 512, 13, 13]           1,024\n",
            "             ReLU-58          [-1, 512, 13, 13]               0\n",
            "       BasicBlock-59          [-1, 512, 13, 13]               0\n",
            "           Conv2d-60          [-1, 512, 13, 13]       2,359,296\n",
            "      BatchNorm2d-61          [-1, 512, 13, 13]           1,024\n",
            "             ReLU-62          [-1, 512, 13, 13]               0\n",
            "           Conv2d-63          [-1, 512, 13, 13]       2,359,296\n",
            "      BatchNorm2d-64          [-1, 512, 13, 13]           1,024\n",
            "             ReLU-65          [-1, 512, 13, 13]               0\n",
            "       BasicBlock-66          [-1, 512, 13, 13]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                 [-1, 1000]         513,000\n",
            "================================================================\n",
            "Total params: 11,689,512\n",
            "Trainable params: 11,689,512\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.98\n",
            "Forward/backward pass size (MB): 216.54\n",
            "Params size (MB): 44.59\n",
            "Estimated Total Size (MB): 263.12\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNcR7TvKkhge"
      },
      "source": [
        "#"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTEoZW5d12sn"
      },
      "source": [
        "# my stategy \n",
        "first load resnet 18 model pretrained=true and include top=false\n",
        "\n",
        "> now create a hooks for extracting feature now need to create multiple hooks\n",
        "as we are going to get multiple layers feature vector,\n",
        "once feature vector are collected we will think about rest of part\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddXKPGiV2cAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f72058f-6ece-4d7d-aa90-eeacbf3a45c4"
      },
      "source": [
        "dir(resnet18)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T_destination',\n",
              " '__annotations__',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_apply',\n",
              " '_backward_hooks',\n",
              " '_buffers',\n",
              " '_call_impl',\n",
              " '_forward_hooks',\n",
              " '_forward_impl',\n",
              " '_forward_pre_hooks',\n",
              " '_get_backward_hooks',\n",
              " '_get_name',\n",
              " '_is_full_backward_hook',\n",
              " '_load_from_state_dict',\n",
              " '_load_state_dict_pre_hooks',\n",
              " '_make_layer',\n",
              " '_maybe_warn_non_full_backward_hook',\n",
              " '_modules',\n",
              " '_named_members',\n",
              " '_non_persistent_buffers_set',\n",
              " '_norm_layer',\n",
              " '_parameters',\n",
              " '_register_load_state_dict_pre_hook',\n",
              " '_register_state_dict_hook',\n",
              " '_replicate_for_data_parallel',\n",
              " '_save_to_state_dict',\n",
              " '_slow_forward',\n",
              " '_state_dict_hooks',\n",
              " '_version',\n",
              " 'add_module',\n",
              " 'apply',\n",
              " 'avgpool',\n",
              " 'base_width',\n",
              " 'bfloat16',\n",
              " 'bn1',\n",
              " 'buffers',\n",
              " 'children',\n",
              " 'conv1',\n",
              " 'cpu',\n",
              " 'cuda',\n",
              " 'dilation',\n",
              " 'double',\n",
              " 'dump_patches',\n",
              " 'eval',\n",
              " 'extra_repr',\n",
              " 'fc',\n",
              " 'float',\n",
              " 'forward',\n",
              " 'get_buffer',\n",
              " 'get_parameter',\n",
              " 'get_submodule',\n",
              " 'groups',\n",
              " 'half',\n",
              " 'inplanes',\n",
              " 'layer1',\n",
              " 'layer2',\n",
              " 'layer3',\n",
              " 'layer4',\n",
              " 'load_state_dict',\n",
              " 'maxpool',\n",
              " 'modules',\n",
              " 'named_buffers',\n",
              " 'named_children',\n",
              " 'named_modules',\n",
              " 'named_parameters',\n",
              " 'parameters',\n",
              " 'register_backward_hook',\n",
              " 'register_buffer',\n",
              " 'register_forward_hook',\n",
              " 'register_forward_pre_hook',\n",
              " 'register_full_backward_hook',\n",
              " 'register_parameter',\n",
              " 'relu',\n",
              " 'requires_grad_',\n",
              " 'share_memory',\n",
              " 'state_dict',\n",
              " 'to',\n",
              " 'to_empty',\n",
              " 'train',\n",
              " 'training',\n",
              " 'type',\n",
              " 'xpu',\n",
              " 'zero_grad']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA_XyKlv_fXq"
      },
      "source": [
        "net = models.resnet18(pretrained=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcQ2OQOoU-Yx"
      },
      "source": [
        "class ReductionResNet():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self):\n",
        "        pass\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPWOPH2rVw2g"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvMXjmV6nYlT"
      },
      "source": [
        "# performming feature extraction using torchextractor package\n",
        "\n",
        "!pip install git+https://github.com/antoinebrl/torchextractor.git "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hNNow9unks2",
        "outputId": "62595b87-cdd1-4fbb-925d-f70a9b7f4f6e"
      },
      "source": [
        "!pip install git+https://github.com/antoinebrl/torchextractor.git "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/antoinebrl/torchextractor.git\n",
            "  Cloning https://github.com/antoinebrl/torchextractor.git to /tmp/pip-req-build-uyebs0m6\n",
            "  Running command git clone -q https://github.com/antoinebrl/torchextractor.git /tmp/pip-req-build-uyebs0m6\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchextractor==0.3.0) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchextractor==0.3.0) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->torchextractor==0.3.0) (3.7.4.3)\n",
            "Building wheels for collected packages: torchextractor\n",
            "  Building wheel for torchextractor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchextractor: filename=torchextractor-0.3.0-py3-none-any.whl size=10701 sha256=773ea0cf9d26aec68e54b258ba2d77cfb427c50e6d5fd38f80cdebbfc816cd7d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v5of5w5s/wheels/2c/49/d6/f6c15485117e7367197186707bedb9798fad079091debfadef\n",
            "Successfully built torchextractor\n",
            "Installing collected packages: torchextractor\n",
            "Successfully installed torchextractor-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFkcxS5_a4xe"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchextractor as tx\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "model = tx.Extractor(model, [\"layer1\", \"layer2\", \"layer3\", \"layer4\"])\n",
        "# dummy_input = torch.rand(7, 3, 416, 416)\n",
        "# model_output, features = model(dummy_input)\n",
        "# feature_shapes = {name: f.shape for name, f in features.items()}\n",
        "# print(feature_shapes)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePuEkvtJnpEv"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HulA8wfznuR3",
        "outputId": "eb4cf2ea-851c-491a-f3e4-fdc5cd611245"
      },
      "source": [
        "model_output, features = model(image_loader(data_transforms, FILENAME))\n",
        "feature_shapes = {name: f.shape for name, f in features.items()}\n",
        "feature_shapes"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layer1': torch.Size([1, 64, 104, 104]),\n",
              " 'layer2': torch.Size([1, 128, 52, 52]),\n",
              " 'layer3': torch.Size([1, 256, 26, 26]),\n",
              " 'layer4': torch.Size([1, 512, 13, 13])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A9EcHL9pb1s",
        "outputId": "21a6410f-1595-4dc4-f619-e3d56c1c4599"
      },
      "source": [
        "layer1 = features.get('layer1')\n",
        "layer2 = features.get('layer2')\n",
        "layer3 = features.get('layer3')\n",
        "layer4 = features.get('layer4')\n",
        "\n",
        "layer1.shape, layer2.shape, layer3.shape, layer4.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 64, 104, 104]),\n",
              " torch.Size([1, 128, 52, 52]),\n",
              " torch.Size([1, 256, 26, 26]),\n",
              " torch.Size([1, 512, 13, 13]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXCLF9tCpkS1",
        "outputId": "6941bc16-856d-4950-92fb-888255ec8c70"
      },
      "source": [
        "# IGNORE: the step is needed but done in a wrong way.\n",
        "l1 = torch.repeat_interleave(layer1, 8, dim=1)\n",
        "l2 = torch.repeat_interleave(layer2, 4, dim=1)\n",
        "l3 = torch.repeat_interleave(layer3, 2, dim=1)\n",
        "l4 = layer4\n",
        "l1.shape, l2.shape, l3.shape, l4.shape,"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 512, 104, 104]),\n",
              " torch.Size([1, 512, 52, 52]),\n",
              " torch.Size([1, 512, 26, 26]),\n",
              " torch.Size([1, 512, 13, 13]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_u6Ph8q1NY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ef1908-9c8a-4d48-e390-53acac26b904"
      },
      "source": [
        "# reduction operation reducing size 4d tensor to 1d tensor\n",
        "aap = torch.nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "\n",
        "l1aap = aap(layer1)\n",
        "l2aap = aap(layer2)\n",
        "l3aap = aap(layer3)\n",
        "l4aap = aap(layer4)\n",
        "\n",
        "print(l1aap.shape, l2aap.shape, l3aap.shape, l4aap.shape)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 64, 1, 1]) torch.Size([1, 128, 1, 1]) torch.Size([1, 256, 1, 1]) torch.Size([1, 512, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dy8nkSXKyYW",
        "outputId": "1b073da4-c98c-4f54-8739-336c3fb3b901"
      },
      "source": [
        "lf = l1aap.reshape([-1])\n",
        "lf = lf.repeat_interleave(8)\n",
        "lf.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLEYq1mnJHO5"
      },
      "source": [
        "# IMPORTANT: the block works as needed in the  model, don't DELETE\n",
        "\n",
        "adaptive_avg_pool_2d = torch.nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "\n",
        "l1 = adaptive_avg_pool_2d(layer1).reshape([-1]).repeat_interleave(8).unsqueeze(0)\n",
        "l2 = adaptive_avg_pool_2d(layer2).reshape([-1]).repeat_interleave(4).unsqueeze(0)\n",
        "l3 = adaptive_avg_pool_2d(layer3).reshape([-1]).repeat_interleave(2).unsqueeze(0)\n",
        "l4 = adaptive_avg_pool_2d(layer4).reshape([-1]).repeat_interleave(1).unsqueeze(0)\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCNJ3sF7lLmp",
        "outputId": "99b1f64c-b223-4232-ddfa-31643db2c39c"
      },
      "source": [
        "embedding = torch.cat((l1,l2,l3,l4), 0).mean(0)\n",
        "embedding.shape"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsSxqnscPlM_",
        "outputId": "55d2e3b0-44a4-4254-f3aa-2d48c16baa92"
      },
      "source": [
        "#  Not working\n",
        "# intermediate_embedding = torch.cat(l1,l2,l3,l4)\n",
        "# intermediate_embedding\n",
        "ll1 = l1.unsqueeze(0)\n",
        "ll1, l1"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.3472, 0.3472, 0.3472, 0.3472, 0.3472, 0.3472, 0.3472, 0.3472, 0.4069,\n",
              "          0.4069, 0.4069, 0.4069, 0.4069, 0.4069, 0.4069, 0.4069, 0.2178, 0.2178,\n",
              "          0.2178, 0.2178, 0.2178, 0.2178, 0.2178, 0.2178, 0.3109, 0.3109, 0.3109,\n",
              "          0.3109, 0.3109, 0.3109, 0.3109, 0.3109, 0.5679, 0.5679, 0.5679, 0.5679,\n",
              "          0.5679, 0.5679, 0.5679, 0.5679, 0.2976, 0.2976, 0.2976, 0.2976, 0.2976,\n",
              "          0.2976, 0.2976, 0.2976, 0.2942, 0.2942, 0.2942, 0.2942, 0.2942, 0.2942,\n",
              "          0.2942, 0.2942, 0.2345, 0.2345, 0.2345, 0.2345, 0.2345, 0.2345, 0.2345,\n",
              "          0.2345, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888,\n",
              "          0.2144, 0.2144, 0.2144, 0.2144, 0.2144, 0.2144, 0.2144, 0.2144, 0.4910,\n",
              "          0.4910, 0.4910, 0.4910, 0.4910, 0.4910, 0.4910, 0.4910, 0.7148, 0.7148,\n",
              "          0.7148, 0.7148, 0.7148, 0.7148, 0.7148, 0.7148, 0.2212, 0.2212, 0.2212,\n",
              "          0.2212, 0.2212, 0.2212, 0.2212, 0.2212, 0.2197, 0.2197, 0.2197, 0.2197,\n",
              "          0.2197, 0.2197, 0.2197, 0.2197, 0.3543, 0.3543, 0.3543, 0.3543, 0.3543,\n",
              "          0.3543, 0.3543, 0.3543, 0.2549, 0.2549, 0.2549, 0.2549, 0.2549, 0.2549,\n",
              "          0.2549, 0.2549, 0.4042, 0.4042, 0.4042, 0.4042, 0.4042, 0.4042, 0.4042,\n",
              "          0.4042, 0.4533, 0.4533, 0.4533, 0.4533, 0.4533, 0.4533, 0.4533, 0.4533,\n",
              "          0.1481, 0.1481, 0.1481, 0.1481, 0.1481, 0.1481, 0.1481, 0.1481, 0.3843,\n",
              "          0.3843, 0.3843, 0.3843, 0.3843, 0.3843, 0.3843, 0.3843, 0.2233, 0.2233,\n",
              "          0.2233, 0.2233, 0.2233, 0.2233, 0.2233, 0.2233, 0.5603, 0.5603, 0.5603,\n",
              "          0.5603, 0.5603, 0.5603, 0.5603, 0.5603, 0.4859, 0.4859, 0.4859, 0.4859,\n",
              "          0.4859, 0.4859, 0.4859, 0.4859, 0.3429, 0.3429, 0.3429, 0.3429, 0.3429,\n",
              "          0.3429, 0.3429, 0.3429, 0.1460, 0.1460, 0.1460, 0.1460, 0.1460, 0.1460,\n",
              "          0.1460, 0.1460, 0.6514, 0.6514, 0.6514, 0.6514, 0.6514, 0.6514, 0.6514,\n",
              "          0.6514, 0.3920, 0.3920, 0.3920, 0.3920, 0.3920, 0.3920, 0.3920, 0.3920,\n",
              "          0.4271, 0.4271, 0.4271, 0.4271, 0.4271, 0.4271, 0.4271, 0.4271, 0.5339,\n",
              "          0.5339, 0.5339, 0.5339, 0.5339, 0.5339, 0.5339, 0.5339, 0.3523, 0.3523,\n",
              "          0.3523, 0.3523, 0.3523, 0.3523, 0.3523, 0.3523, 0.5794, 0.5794, 0.5794,\n",
              "          0.5794, 0.5794, 0.5794, 0.5794, 0.5794, 0.4650, 0.4650, 0.4650, 0.4650,\n",
              "          0.4650, 0.4650, 0.4650, 0.4650, 0.3128, 0.3128, 0.3128, 0.3128, 0.3128,\n",
              "          0.3128, 0.3128, 0.3128, 0.2326, 0.2326, 0.2326, 0.2326, 0.2326, 0.2326,\n",
              "          0.2326, 0.2326, 0.5583, 0.5583, 0.5583, 0.5583, 0.5583, 0.5583, 0.5583,\n",
              "          0.5583, 0.1762, 0.1762, 0.1762, 0.1762, 0.1762, 0.1762, 0.1762, 0.1762,\n",
              "          0.4948, 0.4948, 0.4948, 0.4948, 0.4948, 0.4948, 0.4948, 0.4948, 0.3039,\n",
              "          0.3039, 0.3039, 0.3039, 0.3039, 0.3039, 0.3039, 0.3039, 0.6583, 0.6583,\n",
              "          0.6583, 0.6583, 0.6583, 0.6583, 0.6583, 0.6583, 0.6942, 0.6942, 0.6942,\n",
              "          0.6942, 0.6942, 0.6942, 0.6942, 0.6942, 0.4147, 0.4147, 0.4147, 0.4147,\n",
              "          0.4147, 0.4147, 0.4147, 0.4147, 0.5593, 0.5593, 0.5593, 0.5593, 0.5593,\n",
              "          0.5593, 0.5593, 0.5593, 0.7035, 0.7035, 0.7035, 0.7035, 0.7035, 0.7035,\n",
              "          0.7035, 0.7035, 0.5843, 0.5843, 0.5843, 0.5843, 0.5843, 0.5843, 0.5843,\n",
              "          0.5843, 0.2905, 0.2905, 0.2905, 0.2905, 0.2905, 0.2905, 0.2905, 0.2905,\n",
              "          0.4839, 0.4839, 0.4839, 0.4839, 0.4839, 0.4839, 0.4839, 0.4839, 0.9315,\n",
              "          0.9315, 0.9315, 0.9315, 0.9315, 0.9315, 0.9315, 0.9315, 0.6673, 0.6673,\n",
              "          0.6673, 0.6673, 0.6673, 0.6673, 0.6673, 0.6673, 0.4045, 0.4045, 0.4045,\n",
              "          0.4045, 0.4045, 0.4045, 0.4045, 0.4045, 0.5274, 0.5274, 0.5274, 0.5274,\n",
              "          0.5274, 0.5274, 0.5274, 0.5274, 1.0036, 1.0036, 1.0036, 1.0036, 1.0036,\n",
              "          1.0036, 1.0036, 1.0036, 0.6179, 0.6179, 0.6179, 0.6179, 0.6179, 0.6179,\n",
              "          0.6179, 0.6179, 0.3702, 0.3702, 0.3702, 0.3702, 0.3702, 0.3702, 0.3702,\n",
              "          0.3702, 0.3505, 0.3505, 0.3505, 0.3505, 0.3505, 0.3505, 0.3505, 0.3505,\n",
              "          0.2942, 0.2942, 0.2942, 0.2942, 0.2942, 0.2942, 0.2942, 0.2942, 0.4221,\n",
              "          0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.2488, 0.2488,\n",
              "          0.2488, 0.2488, 0.2488, 0.2488, 0.2488, 0.2488, 0.3450, 0.3450, 0.3450,\n",
              "          0.3450, 0.3450, 0.3450, 0.3450, 0.3450, 0.5031, 0.5031, 0.5031, 0.5031,\n",
              "          0.5031, 0.5031, 0.5031, 0.5031, 0.6818, 0.6818, 0.6818, 0.6818, 0.6818,\n",
              "          0.6818, 0.6818, 0.6818, 0.5375, 0.5375, 0.5375, 0.5375, 0.5375, 0.5375,\n",
              "          0.5375, 0.5375, 0.9146, 0.9146, 0.9146, 0.9146, 0.9146, 0.9146, 0.9146,\n",
              "          0.9146, 0.2424, 0.2424, 0.2424, 0.2424, 0.2424, 0.2424, 0.2424, 0.2424,\n",
              "          0.7534, 0.7534, 0.7534, 0.7534, 0.7534, 0.7534, 0.7534, 0.7534]],\n",
              "        grad_fn=<UnsqueezeBackward0>),\n",
              " tensor([0.3472, 0.3472, 0.3472, 0.3472, 0.3472, 0.3472, 0.3472, 0.3472, 0.4069,\n",
              "         0.4069, 0.4069, 0.4069, 0.4069, 0.4069, 0.4069, 0.4069, 0.2178, 0.2178,\n",
              "         0.2178, 0.2178, 0.2178, 0.2178, 0.2178, 0.2178, 0.3109, 0.3109, 0.3109,\n",
              "         0.3109, 0.3109, 0.3109, 0.3109, 0.3109, 0.5679, 0.5679, 0.5679, 0.5679,\n",
              "         0.5679, 0.5679, 0.5679, 0.5679, 0.2976, 0.2976, 0.2976, 0.2976, 0.2976,\n",
              "         0.2976, 0.2976, 0.2976, 0.2942, 0.2942, 0.2942, 0.2942, 0.2942, 0.2942,\n",
              "         0.2942, 0.2942, 0.2345, 0.2345, 0.2345, 0.2345, 0.2345, 0.2345, 0.2345,\n",
              "         0.2345, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888,\n",
              "         0.2144, 0.2144, 0.2144, 0.2144, 0.2144, 0.2144, 0.2144, 0.2144, 0.4910,\n",
              "         0.4910, 0.4910, 0.4910, 0.4910, 0.4910, 0.4910, 0.4910, 0.7148, 0.7148,\n",
              "         0.7148, 0.7148, 0.7148, 0.7148, 0.7148, 0.7148, 0.2212, 0.2212, 0.2212,\n",
              "         0.2212, 0.2212, 0.2212, 0.2212, 0.2212, 0.2197, 0.2197, 0.2197, 0.2197,\n",
              "         0.2197, 0.2197, 0.2197, 0.2197, 0.3543, 0.3543, 0.3543, 0.3543, 0.3543,\n",
              "         0.3543, 0.3543, 0.3543, 0.2549, 0.2549, 0.2549, 0.2549, 0.2549, 0.2549,\n",
              "         0.2549, 0.2549, 0.4042, 0.4042, 0.4042, 0.4042, 0.4042, 0.4042, 0.4042,\n",
              "         0.4042, 0.4533, 0.4533, 0.4533, 0.4533, 0.4533, 0.4533, 0.4533, 0.4533,\n",
              "         0.1481, 0.1481, 0.1481, 0.1481, 0.1481, 0.1481, 0.1481, 0.1481, 0.3843,\n",
              "         0.3843, 0.3843, 0.3843, 0.3843, 0.3843, 0.3843, 0.3843, 0.2233, 0.2233,\n",
              "         0.2233, 0.2233, 0.2233, 0.2233, 0.2233, 0.2233, 0.5603, 0.5603, 0.5603,\n",
              "         0.5603, 0.5603, 0.5603, 0.5603, 0.5603, 0.4859, 0.4859, 0.4859, 0.4859,\n",
              "         0.4859, 0.4859, 0.4859, 0.4859, 0.3429, 0.3429, 0.3429, 0.3429, 0.3429,\n",
              "         0.3429, 0.3429, 0.3429, 0.1460, 0.1460, 0.1460, 0.1460, 0.1460, 0.1460,\n",
              "         0.1460, 0.1460, 0.6514, 0.6514, 0.6514, 0.6514, 0.6514, 0.6514, 0.6514,\n",
              "         0.6514, 0.3920, 0.3920, 0.3920, 0.3920, 0.3920, 0.3920, 0.3920, 0.3920,\n",
              "         0.4271, 0.4271, 0.4271, 0.4271, 0.4271, 0.4271, 0.4271, 0.4271, 0.5339,\n",
              "         0.5339, 0.5339, 0.5339, 0.5339, 0.5339, 0.5339, 0.5339, 0.3523, 0.3523,\n",
              "         0.3523, 0.3523, 0.3523, 0.3523, 0.3523, 0.3523, 0.5794, 0.5794, 0.5794,\n",
              "         0.5794, 0.5794, 0.5794, 0.5794, 0.5794, 0.4650, 0.4650, 0.4650, 0.4650,\n",
              "         0.4650, 0.4650, 0.4650, 0.4650, 0.3128, 0.3128, 0.3128, 0.3128, 0.3128,\n",
              "         0.3128, 0.3128, 0.3128, 0.2326, 0.2326, 0.2326, 0.2326, 0.2326, 0.2326,\n",
              "         0.2326, 0.2326, 0.5583, 0.5583, 0.5583, 0.5583, 0.5583, 0.5583, 0.5583,\n",
              "         0.5583, 0.1762, 0.1762, 0.1762, 0.1762, 0.1762, 0.1762, 0.1762, 0.1762,\n",
              "         0.4948, 0.4948, 0.4948, 0.4948, 0.4948, 0.4948, 0.4948, 0.4948, 0.3039,\n",
              "         0.3039, 0.3039, 0.3039, 0.3039, 0.3039, 0.3039, 0.3039, 0.6583, 0.6583,\n",
              "         0.6583, 0.6583, 0.6583, 0.6583, 0.6583, 0.6583, 0.6942, 0.6942, 0.6942,\n",
              "         0.6942, 0.6942, 0.6942, 0.6942, 0.6942, 0.4147, 0.4147, 0.4147, 0.4147,\n",
              "         0.4147, 0.4147, 0.4147, 0.4147, 0.5593, 0.5593, 0.5593, 0.5593, 0.5593,\n",
              "         0.5593, 0.5593, 0.5593, 0.7035, 0.7035, 0.7035, 0.7035, 0.7035, 0.7035,\n",
              "         0.7035, 0.7035, 0.5843, 0.5843, 0.5843, 0.5843, 0.5843, 0.5843, 0.5843,\n",
              "         0.5843, 0.2905, 0.2905, 0.2905, 0.2905, 0.2905, 0.2905, 0.2905, 0.2905,\n",
              "         0.4839, 0.4839, 0.4839, 0.4839, 0.4839, 0.4839, 0.4839, 0.4839, 0.9315,\n",
              "         0.9315, 0.9315, 0.9315, 0.9315, 0.9315, 0.9315, 0.9315, 0.6673, 0.6673,\n",
              "         0.6673, 0.6673, 0.6673, 0.6673, 0.6673, 0.6673, 0.4045, 0.4045, 0.4045,\n",
              "         0.4045, 0.4045, 0.4045, 0.4045, 0.4045, 0.5274, 0.5274, 0.5274, 0.5274,\n",
              "         0.5274, 0.5274, 0.5274, 0.5274, 1.0036, 1.0036, 1.0036, 1.0036, 1.0036,\n",
              "         1.0036, 1.0036, 1.0036, 0.6179, 0.6179, 0.6179, 0.6179, 0.6179, 0.6179,\n",
              "         0.6179, 0.6179, 0.3702, 0.3702, 0.3702, 0.3702, 0.3702, 0.3702, 0.3702,\n",
              "         0.3702, 0.3505, 0.3505, 0.3505, 0.3505, 0.3505, 0.3505, 0.3505, 0.3505,\n",
              "         0.2942, 0.2942, 0.2942, 0.2942, 0.2942, 0.2942, 0.2942, 0.2942, 0.4221,\n",
              "         0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.2488, 0.2488,\n",
              "         0.2488, 0.2488, 0.2488, 0.2488, 0.2488, 0.2488, 0.3450, 0.3450, 0.3450,\n",
              "         0.3450, 0.3450, 0.3450, 0.3450, 0.3450, 0.5031, 0.5031, 0.5031, 0.5031,\n",
              "         0.5031, 0.5031, 0.5031, 0.5031, 0.6818, 0.6818, 0.6818, 0.6818, 0.6818,\n",
              "         0.6818, 0.6818, 0.6818, 0.5375, 0.5375, 0.5375, 0.5375, 0.5375, 0.5375,\n",
              "         0.5375, 0.5375, 0.9146, 0.9146, 0.9146, 0.9146, 0.9146, 0.9146, 0.9146,\n",
              "         0.9146, 0.2424, 0.2424, 0.2424, 0.2424, 0.2424, 0.2424, 0.2424, 0.2424,\n",
              "         0.7534, 0.7534, 0.7534, 0.7534, 0.7534, 0.7534, 0.7534, 0.7534],\n",
              "        grad_fn=<IndexSelectBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z7TpAYBqZ0e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwqyii9MIGZ0"
      },
      "source": [
        "#  x = torch.tensor([[1], [2], [3]])\n",
        "# >>> x.size()\n",
        "# torch.Size([3, 1])\n",
        "# >>> x.expand(3, 4)\n",
        "# tensor([[ 1,  1,  1,  1],\n",
        "#         [ 2,  2,  2,  2],\n",
        "#         [ 3,  3,  3,  3]])\n",
        "# >>> x.expand(-1, 4)   # -1 means not changing the size of that dimension\n",
        "# tensor([[ 1,  1,  1,  1],\n",
        "#         [ 2,  2,  2,  2],\n",
        "#         [ 3,  3,  3,  3]])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCNMPMQPFE3C"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ominlN0wt-H0"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZVtc27Ct_Xk"
      },
      "source": [
        "l1rl = torch.repeat_interleave(output, 8, dim=1)\n",
        "l1rl.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbx6f9rK09WP"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyPVbzqG2gbf"
      },
      "source": [
        "output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpvG6ZisEG5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34891e4-bd05-4e0d-db29-11fb68154933"
      },
      "source": [
        "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "res = cos(embedding, embedding)\n",
        "res"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUmiaP-dseC4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}